---
title: '[Youtube] Why the internet went down for 2.5 hours yesterday '
date: '2025-06-14'
tags: ['youtube', 'theo', 'internet']
draft: false
summary: '최근 발생한 대규모 인터넷 서비스 장애는 Discord, Google Cloud, Spotify, Google, Twitch, Character AI, Google Meet, Rocket League, Nest 등 광범위한 서비스에 영향을 미쳤으며, 이는 인터넷 인프라의 상호 의존성과 특정 핵심 제공업체에 대한 의존성의 위험성을 명확히 보여주었습니다. 특히 Cloudflare의 핵심 서비스인 Worker KV 장애가 이번 사태의 주요 원인으로 지목되었으며, 이 서비스의 숨겨진 Google Cloud 종속성이 밝혀지며 업계에 큰 충격을 주었습니다.'
images:
  - 'https://img.youtube.com/vi/ECOFX9Ss0KY/maxresdefault.jpg'
---

[![Why the internet went down for 2.5 hours yesterday](https://img.youtube.com/vi/ECOFX9Ss0KY/maxresdefault.jpg)](https://www.youtube.com/watch?v=ECOFX9Ss0KY)

> 출처 : [Theo - t3․gg](https://www.youtube.com/@t3dotgg)

> notebookLM을 이용해 작성되었습니다.

---

## 인터넷 서비스 중단 사태 브리핑 (2025년 6월 12일)

### 1. 클라우드플레어의 서비스 및 핵심 의존성

클라우드플레어는 크게 세 가지 시대를 거쳐 발전했습니다: DDoS 보호, 개발자 플랫폼, 제로 트러스트(Zero Trust). 이번 사태의 핵심은 **개발자 플랫폼**에 있으며, 특히 **Worker KV** 서비스가 중요하게 작용했습니다.

- **Worker KV**: 클라우드플레어의 핵심 키-값 스토어(key-value store)로, 수많은 다른 클라우드플레어 서비스의 기반이 됩니다. Worker KV가 다운되면 대부분의 클라우드플레어 서비스에 **연쇄적인 장애(cascading failure)가 발생**하는 경향이 있습니다. 이는 마치 컴퓨터에서 RAM을 제거하는 것과 유사한 영향을 미 미칩니다.

### 2. 장애의 원인 및 영향

- **원인**: 이번 중단의 주요 원인은 **Worker KV 서비스가 사용하는 기반 스토리지 인프라의 장애**였습니다. 이 스토리지 인프라는 구성, 인증, 자산 전달 등 클라우드플레어의 다양한 제품에 필수적인 종속성이었습니다.
- **예상치 못한 구글 클라우드 의존성**: 놀랍게도, **Worker KV는 콜드 스토리지(cold storage) 또는 장기 스토리지 솔루션에 대해 구글 클라우드에 의존하고 있었습니다**. 이는 클라우드플레어가 자체 인프라를 사용하는 것으로 알려져 있었기 때문에, 많은 사람들에게 알려지지 않았던 사실입니다.
- **영향을 받은 서비스**:
  - **Worker KV 요청의 90% 이상 실패**.
  - **클라우드플레어 대시보드를 포함한 다수의 핵심 서비스 중단**: Warp, Access Gateway, Images, Stream, Workers AI, Turnstyle, Challenges 등.
  - T3 Chat의 경우, OAuth 레이어가 클라우드플레어를 통해 관리되었기 때문에 영향을 받았습니다.
  - **데이터 손실 여부**: 클라우드플레어는 직접적인 데이터 손실은 없었다고 밝혔지만, 중단 중에 예상되었던 분석 데이터 등은 손실되었을 수 있습니다.
  - **공격 아님**: 이번 사건은 해킹이나 보안 이벤트의 결과가 아니었습니다.

### 3. 클라우드플레어의 대응 및 투명성

- **전적인 책임 인정**: 클라우드플레어의 CTO인 Dane은 이번 중단에 대해 **전적인 책임을 인정**했으며, 이러한 수준의 책임감은 대기업에서 매우 드문 일로 평가받았습니다.
- **신속한 초기 대응**: 중단 시작 후 16분 이내에 **근본 원인이 식별**되었고, 최고 심각도인 P0으로 격상되었습니다.
- **병렬 복구 노력**:
  - 조사 진행과 동시에, **Worker KV 의존성을 제거할 대안을 모색**하고 실패한 종속성에서 벗어날 방법을 탐색했습니다.
  - Worker KV에 대한 부하를 줄이기 위해 일시적으로 서비스를 저하시키는 조치를 취했습니다.
  - 내부적으로 자체 Worker KV 인스턴스를 가동하여 자체 서비스의 복구를 시도했습니다.
- **높은 투명성**: 장애 발생 후 불과 4시간 이내에 상세한 보고서를 발표했으며, 실제 소프트웨어에서 발생한 **오류율 차트까지 공유**하는 등 매우 높은 수준의 투명성을 보여주었습니다.

### 4. 향후 계획 및 재발 방지 노력

클라우드플레어는 이번 사건을 계기로 서비스 복원력을 강화하기 위한 여러 조치를 가속화할 것이라고 밝혔습니다.

- **Worker KV 스토리지 인프라의 이중화 개선**: Worker KV 스토리지 인프라의 중복성을 높이는 작업을 가속화합니다.
- **단일 제공업체 의존성 제거**: **구글 클라우드와 같은 단일 스토리지 인프라 제공업체에 대한 의존성을 제거**할 계획입니다.
  - 현재 Worker KV는 중앙 데이터 스토어를 **R2(클라우드플레어의 S3 대체 서비스)**로 전환하는 중입니다.
- **개별 제품의 복원력 강화**: Worker KV의 장애가 다른 모든 서비스로 파급되지 않도록 각 제품이 단일 실패 지점에 견딜 수 있도록 만들 것입니다.
- **점진적 네임스페이스 재활성화 도구 구현**: 장애 발생 시 모든 것을 끄는 대신, 특정 네임스페이스(예: 내부 서비스)를 수동으로 재활성화할 수 있는 도구를 도입하여 통제력을 높일 예정입니다. 이는 캐시 계층이 다운되었을 때 발생하는 부하 증가를 관리하는 데 중요합니다.

---
